{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo 'Parkinsons_VGRF_Spatiotemporal_Diagnosis_v2' not found. Cloning a fresh copy...\n",
      "Initializing sparse checkout for 'src' directory...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "repo_url = 'https://github.com/alxxtexxr/Parkinsons_VGRF_Spatiotemporal_Diagnosis_v2.git'\n",
    "repo_name = repo_url.rstrip('.git').split('/')[-1] # Extract repo name from URL\n",
    "root_dir = '/content'\n",
    "repo_dir = os.path.join(root_dir, repo_name)\n",
    "\n",
    "if os.path.isdir(repo_dir):\n",
    "    print(f\"Repo '{repo_name}' already exists. Updating to the latest version...\")\n",
    "    os.chdir(repo_dir)\n",
    "    subprocess.run(['git', 'reset', '--hard', 'origin/master'], check=True) # Reset to match remote\n",
    "    subprocess.run(['git', 'pull'], check=True) # Pull latest changes\n",
    "else:\n",
    "    print(f\"Repo '{repo_name}' not found. Cloning a fresh copy...\")\n",
    "    os.chdir(root_dir)\n",
    "    subprocess.run(['git', 'clone', '--no-checkout', repo_url], check=True)\n",
    "    os.chdir(repo_dir)\n",
    "    print(\"Initializing sparse checkout for 'src' directory...\")\n",
    "    subprocess.run(['git', 'sparse-checkout', 'init', '--cone'], check=True)\n",
    "    subprocess.run(['git', 'sparse-checkout', 'set', 'src'], check=True)\n",
    "    subprocess.run(['git', 'checkout'], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.3/324.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install -q tsai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from src.utils import (\n",
    "    print_h, eval_window, eval_person_majority_voting,\n",
    "    set_seed,\n",
    ")\n",
    "from src.models.RNNInceptionTime import RNNInceptionTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_output(output, eval_output):\n",
    "    for metric in ['acc', 'f1', 'precision', 'recall', 'cm']:\n",
    "        output[metric]['folds'] += [eval_output[metric]]\n",
    "        if metric != 'cm':\n",
    "            output[metric]['avg'] = np.mean(output[metric]['folds'])\n",
    "            output[metric]['std'] = np.std(output[metric]['folds'])\n",
    "    return output\n",
    "\n",
    "class HardMoE(torch.nn.Module):\n",
    "    def __init__(self, experts, gate):\n",
    "        super(MoE, self).__init__()\n",
    "        self.experts = torch.nn.ModuleList(experts)\n",
    "        self.gate = gate\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_out = self.gate(x)\n",
    "        gate_out_max_idxs = torch.argmax(gate_out, dim=1)\n",
    "        expert_outs = torch.stack([expert(x) for expert in self.experts], dim=-1)\n",
    "        output = expert_outs[torch.arange(expert_outs.size(0)), :, gate_out_max_idxs]\n",
    "        return output\n",
    "\n",
    "seed = 69\n",
    "k_fold_dir_map = {\n",
    "    'Ga': '/home/mitlab/Documents/alxxtexxr/projects/classification_parkinsons_vgrf/datasets/preprocessed_mixed_loocv_v20240731_val_2/kfold10_window500_stride500_feature16_Ga2',\n",
    "    'Ju': '/home/mitlab/Documents/alxxtexxr/projects/classification_parkinsons_vgrf/datasets/preprocessed_mixed_loocv_v20240731_val_2/kfold10_window500_stride500_feature16_Ju3',\n",
    "    'Si': '/home/mitlab/Documents/alxxtexxr/projects/classification_parkinsons_vgrf/datasets/preprocessed_mixed_loocv_v20240731_val_2/kfold10_window500_stride250_feature16_Si8',\n",
    "}\n",
    "expert_model_dir_map = {\n",
    "    'Ga': '/home/mitlab/Documents/alxxtexxr/projects/classification_parkinsons_vgrf/outputs/train/RNNInceptionTime_Ga_baseline_20240911161249',\n",
    "    'Ju': '/home/mitlab/Documents/alxxtexxr/projects/classification_parkinsons_vgrf/outputs/train/RNNInceptionTime_Ju_baseline_20240911143621',\n",
    "    'Si': '/home/mitlab/Documents/alxxtexxr/projects/classification_parkinsons_vgrf/outputs/train/RNNInceptionTime_Si_baseline_20240911144140'\n",
    "}\n",
    "gate_model_dir = '/home/mitlab/Documents/alxxtexxr/projects/classification_parkinsons_vgrf/outputs/train/RNNInceptionTime_Gate_baseline_20240912075120'\n",
    "\n",
    "k_fold = 10\n",
    "batch_size = 8\n",
    "n_feature = 16\n",
    "n_class = 4\n",
    "window_size = 500\n",
    "max_vgrf_data_len = 25_000\n",
    "device = 'cuda:0'\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "\n",
    "moe_output = {\n",
    "    'acc': { 'folds': [], 'avg': None, 'std': None, },\n",
    "    'f1': { 'folds': [], 'avg': None, 'std': None, },\n",
    "    'precision': { 'folds': [], 'avg': None, 'std': None, },\n",
    "    'recall': { 'folds': [], 'avg': None, 'std': None, },\n",
    "    'cm': { 'folds': [], },\n",
    "    'val_loss': { 'folds': [], },\n",
    "}\n",
    "gate_output = {\n",
    "    'acc': { 'folds': [], 'avg': None, 'std': None, },\n",
    "    'f1': { 'folds': [], 'avg': None, 'std': None, },\n",
    "    'precision': { 'folds': [], 'avg': None, 'std': None, },\n",
    "    'recall': { 'folds': [], 'avg': None, 'std': None, },\n",
    "    'cm': { 'folds': [], },\n",
    "    'val_loss': { 'folds': [], },\n",
    "}\n",
    "expert_outputs = {\n",
    "    'Ga': {\n",
    "        'acc': { 'folds': [], 'avg': None, 'std': None, },\n",
    "        'f1': { 'folds': [], 'avg': None, 'std': None, },\n",
    "        'precision': { 'folds': [], 'avg': None, 'std': None, },\n",
    "        'recall': { 'folds': [], 'avg': None, 'std': None, },\n",
    "        'cm': { 'folds': [], },\n",
    "        'val_loss': { 'folds': [], },\n",
    "    },\n",
    "    'Ju': {\n",
    "        'acc': { 'folds': [], 'avg': None, 'std': None, },\n",
    "        'f1': { 'folds': [], 'avg': None, 'std': None, },\n",
    "        'precision': { 'folds': [], 'avg': None, 'std': None, },\n",
    "        'recall': { 'folds': [], 'avg': None, 'std': None, },\n",
    "        'cm': { 'folds': [], },\n",
    "        'val_loss': { 'folds': [], },\n",
    "    },\n",
    "    'Si': {\n",
    "        'acc': { 'folds': [], 'avg': None, 'std': None, },\n",
    "        'f1': { 'folds': [], 'avg': None, 'std': None, },\n",
    "        'precision': { 'folds': [], 'avg': None, 'std': None, },\n",
    "        'recall': { 'folds': [], 'avg': None, 'std': None, },\n",
    "        'cm': { 'folds': [], },\n",
    "        'val_loss': { 'folds': [], },\n",
    "    },\n",
    "}\n",
    "\n",
    "for i_fold in range(k_fold):\n",
    "    print_h(f\"FOLD-{i_fold+1}\", 128)\n",
    "\n",
    "    study_label_map = {\n",
    "        'Ga': 0,\n",
    "        'Ju': 1,\n",
    "        'Si': 2,\n",
    "    }\n",
    "    \n",
    "    expert_model_map = {\n",
    "        'Ga': RNNInceptionTime(c_in=n_feature, c_out=n_class, seq_len=window_size, bidirectional=True).to(device),\n",
    "        'Ju': RNNInceptionTime(c_in=n_feature, c_out=n_class, seq_len=window_size, bidirectional=True).to(device),\n",
    "        'Si': RNNInceptionTime(c_in=n_feature, c_out=n_class, seq_len=window_size, bidirectional=True).to(device),\n",
    "    }\n",
    "\n",
    "    X_train_window_GaJuSi = torch.empty(0, window_size, n_feature).float()\n",
    "    y_train_window_GaJuSi = torch.empty(0).long()\n",
    "    study_labels_train_window_GaJuSi = torch.empty(0).long()\n",
    "    \n",
    "    X_val_window_GaJuSi = torch.empty(0, window_size, n_feature).float()\n",
    "    y_val_window_GaJuSi = torch.empty(0).long()\n",
    "    study_labels_val_window_GaJuSi = torch.empty(0).long()\n",
    "\n",
    "    X_test_window_GaJuSi = torch.empty(0, window_size, n_feature).float()\n",
    "    y_test_window_GaJuSi = torch.empty(0).long()\n",
    "    study_labels_test_window_GaJuSi = torch.empty(0).long()\n",
    "\n",
    "    X_val_person_GaJuSi = torch.empty(0, max_vgrf_data_len, n_feature).float()\n",
    "    y_val_person_GaJuSi = torch.empty(0).long()\n",
    "    # study_labels_val_person_GaJuSi = torch.empty(0).long()\n",
    "\n",
    "    X_test_person_GaJuSi = torch.empty(0, max_vgrf_data_len, n_feature).float()\n",
    "    y_test_person_GaJuSi = torch.empty(0).long()\n",
    "    # study_labels_test_person_GaJuSi = torch.empty(0).long()\n",
    "\n",
    "    for study, k_fold_dir in k_fold_dir_map.items():\n",
    "        fold_i_dir_name = os.listdir(k_fold_dir)[i_fold]\n",
    "        fold_i_dir = os.path.join(k_fold_dir, fold_i_dir_name)\n",
    "\n",
    "        X_train_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'X_train_window.npy'))).float()\n",
    "        y_train_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'y_train_window.npy'))).long()\n",
    "        study_labels_train_window = torch.tensor([study_label_map[study]] * len(y_train_window)).long()\n",
    "        X_train_window_GaJuSi = torch.cat((X_train_window_GaJuSi, X_train_window), dim=0)\n",
    "        y_train_window_GaJuSi = torch.cat((y_train_window_GaJuSi, y_train_window), dim=0)\n",
    "        study_labels_train_window_GaJuSi = torch.cat((study_labels_train_window_GaJuSi, study_labels_train_window), dim=0)\n",
    "\n",
    "        X_val_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'X_val_window.npy'))).float()\n",
    "        y_val_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'y_val_window.npy'))).long()\n",
    "        study_labels_val_window = torch.tensor([study_label_map[study]] * len(y_val_window)).long()\n",
    "        X_val_window_GaJuSi = torch.cat((X_val_window_GaJuSi, X_val_window), dim=0)\n",
    "        y_val_window_GaJuSi = torch.cat((y_val_window_GaJuSi, y_val_window), dim=0)\n",
    "        study_labels_val_window_GaJuSi = torch.cat((study_labels_val_window_GaJuSi, study_labels_val_window), dim=0)\n",
    "\n",
    "        X_test_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'X_test_window.npy'))).float()\n",
    "        y_test_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'y_test_window.npy'))).long()\n",
    "        study_labels_test_window = torch.tensor([study_label_map[study]] * len(y_test_window)).long()\n",
    "        X_test_window_GaJuSi = torch.cat((X_test_window_GaJuSi, X_test_window), dim=0)\n",
    "        y_test_window_GaJuSi = torch.cat((y_test_window_GaJuSi, y_test_window), dim=0)\n",
    "        study_labels_test_window_GaJuSi = torch.cat((study_labels_test_window_GaJuSi, study_labels_test_window), dim=0)\n",
    "\n",
    "        X_val_person = torch.tensor(np.load(os.path.join(fold_i_dir, f'X_val_person.npy'))).float()\n",
    "        y_val_person = torch.tensor(np.load(os.path.join(fold_i_dir, f'y_val_person.npy'))).long()\n",
    "        X_val_person_GaJuSi = torch.cat((X_val_person_GaJuSi, X_val_person), dim=0)\n",
    "        y_val_person_GaJuSi = torch.cat((y_val_person_GaJuSi, y_val_person), dim=0)\n",
    "\n",
    "        X_test_person = torch.tensor(np.load(os.path.join(fold_i_dir, f'X_test_person.npy'))).float()\n",
    "        y_test_person = torch.tensor(np.load(os.path.join(fold_i_dir, f'y_test_person.npy'))).long()\n",
    "        X_test_person_GaJuSi = torch.cat((X_test_person_GaJuSi, X_test_person), dim=0)\n",
    "        y_test_person_GaJuSi = torch.cat((y_test_person_GaJuSi, y_test_person), dim=0)\n",
    "\n",
    "        train_window_dataset = TensorDataset(X_train_window, y_train_window)\n",
    "        val_window_dataset = TensorDataset(X_val_window, y_val_window)\n",
    "        test_window_dataset = TensorDataset(X_test_window, y_test_window)\n",
    "        \n",
    "        val_person_dataset = TensorDataset(X_val_person, y_val_person)\n",
    "        test_person_dataset = TensorDataset(X_test_person, y_test_person)\n",
    "\n",
    "        train_dataloader = DataLoader(train_window_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_window_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_dataloader = DataLoader(test_window_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        expert_model = expert_model_map[study]\n",
    "\n",
    "        # Load pretrained model\n",
    "        expert_model_dir = expert_model_dir_map[study]\n",
    "        expert_model_i_name = os.listdir(expert_model_dir)[i_fold]\n",
    "        expert_model_i_path = os.path.join(expert_model_dir, expert_model_i_name)\n",
    "        expert_model.load_state_dict(torch.load(expert_model_i_path))\n",
    "\n",
    "        print_h(\"EVALUATION ON PERSON DATA BY MAJORITY VOTING\", 64)\n",
    "        _, acc_person_majority_voting, f1_person_majority_voting, precision_person_majority_voting, recall_person_majority_voting, cm_person_majority_voting = eval_person_majority_voting(expert_model, val_person_dataset, criterion=None, average='weighted',\n",
    "                                                                                                                                                                                                window_size=window_size, debug=False)\n",
    "        print(\"acc:\", acc_person_majority_voting)\n",
    "        print(\"f1:\", f1_person_majority_voting)\n",
    "        print(\"precision:\", precision_person_majority_voting)\n",
    "        print(\"recall:\", recall_person_majority_voting)\n",
    "        print(\"cm:\\n\", np.array(cm_person_majority_voting))\n",
    "        print()\n",
    "\n",
    "        expert_outputs[study] = update_output(expert_outputs[study], {\n",
    "            'acc': acc_person_majority_voting,\n",
    "            'f1': f1_person_majority_voting,\n",
    "            'precision': precision_person_majority_voting,\n",
    "            'recall': recall_person_majority_voting,\n",
    "            'cm': cm_person_majority_voting,\n",
    "        })\n",
    "\n",
    "    print_h(\"GATE\", 96)\n",
    "\n",
    "    # train_window_dataset_GaJuSi = TensorDataset(X_train_window_GaJuSi, y_train_window_GaJuSi)\n",
    "    # val_window_dataset_GaJuSi = TensorDataset(X_val_window_GaJuSi, y_val_window_GaJuSi)\n",
    "    # test_window_dataset_GaJuSi = TensorDataset(X_test_window_GaJuSi, y_test_window_GaJuSi)\n",
    "\n",
    "    train_window_dataset_GaJuSi = TensorDataset(X_train_window_GaJuSi, study_labels_train_window_GaJuSi)\n",
    "    val_window_dataset_GaJuSi = TensorDataset(X_val_window_GaJuSi, study_labels_val_window_GaJuSi)\n",
    "    test_window_dataset_GaJuSi = TensorDataset(X_test_window_GaJuSi, study_labels_test_window_GaJuSi)\n",
    "\n",
    "    train_dataloader_GaJuSi = DataLoader(train_window_dataset_GaJuSi, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader_GaJuSi = DataLoader(val_window_dataset_GaJuSi, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader_GaJuSi = DataLoader(test_window_dataset_GaJuSi, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    gate_model = RNNInceptionTime(c_in=n_feature, c_out=len(study_label_map.keys()), seq_len=window_size, bidirectional=True).to(device)\n",
    "\n",
    "    # Load pretrained model\n",
    "    gate_model_i_name = os.listdir(gate_model_dir)[i_fold]\n",
    "    gate_model_i_path = os.path.join(gate_model_dir, gate_model_i_name)\n",
    "    gate_model.load_state_dict(torch.load(gate_model_i_path))\n",
    "\n",
    "    print_h(\"EVALUATION ON WINDOW DATA\", 64)\n",
    "    \n",
    "    _, acc_window, f1_window, precision_window, recall_window, cm_window = eval_window(gate_model, test_dataloader_GaJuSi, average='weighted')\n",
    "\n",
    "    print(\"acc:\", acc_window)\n",
    "    print(\"f1:\", f1_window)\n",
    "    print(\"precision:\", precision_window)\n",
    "    print(\"recall:\", recall_window)\n",
    "    print(\"cm:\\n\", np.array(cm_window))\n",
    "    print()\n",
    "\n",
    "    gate_output = update_output(gate_output, {\n",
    "        'acc': acc_window,\n",
    "        'f1': f1_window,\n",
    "        'precision': precision_window,\n",
    "        'recall': recall_window,\n",
    "        'cm': cm_window,\n",
    "    })\n",
    "\n",
    "    print_h(\"MoE\", 96)\n",
    "\n",
    "    val_person_dataset_GaJuSi = TensorDataset(X_val_person_GaJuSi, y_val_person_GaJuSi)\n",
    "    test_person_dataset_GaJuSi = TensorDataset(X_test_person_GaJuSi, y_test_person_GaJuSi)\n",
    "\n",
    "    moe_model = MoE(experts=expert_model_map.values(), gate=gate_model)\n",
    "\n",
    "    print_h(\"EVALUATION ON PERSON DATA BY MAJORITY VOTING\", 64)\n",
    "    _, acc_person_majority_voting, f1_person_majority_voting, precision_person_majority_voting, recall_person_majority_voting, cm_person_majority_voting = eval_person_majority_voting(moe_model, val_person_dataset_GaJuSi, criterion=None, average='weighted',\n",
    "                                                                                                                                                                                        window_size=window_size, debug=False)\n",
    "    print(\"acc:\", acc_person_majority_voting)\n",
    "    print(\"f1:\", f1_person_majority_voting)\n",
    "    print(\"precision:\", precision_person_majority_voting)\n",
    "    print(\"recall:\", recall_person_majority_voting)\n",
    "    print(\"cm:\\n\", np.array(cm_person_majority_voting))\n",
    "    print()\n",
    "\n",
    "    moe_output = update_output(moe_output, {\n",
    "        'acc': acc_person_majority_voting,\n",
    "        'f1': f1_person_majority_voting,\n",
    "        'precision': precision_person_majority_voting,\n",
    "        'recall': recall_person_majority_voting,\n",
    "        'cm': cm_person_majority_voting,\n",
    "    })\n",
    "\n",
    "    # for metric in ['acc', 'f1', 'precision', 'recall', 'cm']:\n",
    "    #     moe_output[metric]['folds'] += [moe_eval_output[metric]]\n",
    "    #     if metric != 'cm':\n",
    "    #         moe_output[metric]['avg'] = np.mean(moe_output[metric]['folds'])\n",
    "    #         moe_output[metric]['std'] = np.std(moe_output[metric]['folds'])\n",
    "    \n",
    "    # output['window']['train_loss']['folds'].append(global_train_loss_list)\n",
    "    # output['window']['val_loss']['folds'].append(global_val_loss_window_list)\n",
    "\n",
    "    # break # Test for only 1 fold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
