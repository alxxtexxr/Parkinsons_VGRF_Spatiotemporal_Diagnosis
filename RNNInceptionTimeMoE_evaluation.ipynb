{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo 'Parkinsons_VGRF_Spatiotemporal_Diagnosis_v2' already exists. Updating to the latest version...\n",
      "Now working at repo directory: /content/Parkinsons_VGRF_Spatiotemporal_Diagnosis_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def sparse_clone_repo(repo_url, sparse_dirs, root_dir='.'):\n",
    "    \"\"\"\n",
    "    Clones or updates a Git repository with sparse checkout for specified directories.\n",
    "    Parameters:\n",
    "    - repo_url (str): The URL of the Git repository.\n",
    "    - sparse_dirs (list of str): List of directories to retrieve using sparse checkout.\n",
    "    - root_dir (str): The root directory where the repository will be stored (default is '/content').\n",
    "    \"\"\"\n",
    "    repo_name = repo_url.rstrip('.git').split('/')[-1] # Extract repo name from URL\n",
    "    repo_dir = os.path.join(root_dir, repo_name)\n",
    "\n",
    "    if os.path.isdir(repo_dir):\n",
    "        print(f\"Repo '{repo_name}' already exists. Updating to the latest version...\")\n",
    "        os.chdir(repo_dir)\n",
    "        subprocess.run(['git', 'reset', '--hard', 'origin/master'], check=True) # Reset to match remote\n",
    "        subprocess.run(['git', 'pull'], check=True) # Pull latest changes\n",
    "    else:\n",
    "        print(f\"Repo '{repo_name}' not found. Cloning a fresh copy...\")\n",
    "        os.chdir(root_dir)\n",
    "        subprocess.run(['git', 'clone', '--no-checkout', repo_url], check=True)\n",
    "        os.chdir(repo_dir)\n",
    "        print(f\"Initializing sparse checkout for directories: {', '.join(sparse_dirs)}...\")\n",
    "        subprocess.run(['git', 'sparse-checkout', 'init', '--cone'], check=True)\n",
    "        subprocess.run(['git', 'sparse-checkout', 'set', *sparse_dirs], check=True)\n",
    "        subprocess.run(['git', 'checkout'], check=True)\n",
    "    \n",
    "    print(\"Now working at repo directory:\", os.getcwd())\n",
    "\n",
    "sparse_clone_repo(\n",
    "    repo_url='https://github.com/alxxtexxr/Parkinsons_VGRF_Spatiotemporal_Diagnosis_v2.git',\n",
    "    sparse_dirs=['src', 'data', 'checkpoints'],\n",
    "    root_dir='/content'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.3/324.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m861.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install -q tsai # Required library for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.models.HardMoE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ea8068c5667e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNNInceptionTime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRNNInceptionTime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHardMoE\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHardMoE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src.models.HardMoE'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from src.utils import (\n",
    "    print_h, eval_window, eval_person_majority_voting,\n",
    "    set_seed,\n",
    ")\n",
    "from src.models.RNNInceptionTime import RNNInceptionTime\n",
    "from src.models.HardMoE import HardMoE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 69\n"
     ]
    }
   ],
   "source": [
    "# Project config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 69\n",
    "set_seed(seed)\n",
    "\n",
    "# Model config\n",
    "k_fold_dir_map = {\n",
    "    'Ga': 'data/preprocessed_LOOCV/Ga_k_fold_10_window_500_stride_500_n_feat_16',\n",
    "    'Ju': 'data/preprocessed_LOOCV/Ju_k_fold_10_window_500_stride_500_n_feat_16',\n",
    "    'Si': 'data/preprocessed_LOOCV/Si_k_fold_10_window_500_stride_250_n_feat_16',\n",
    "}\n",
    "expert_model_dir_map = {\n",
    "    'Ga': 'checkpoints/RNNInceptionTime_Ga_k_fold_10',\n",
    "    'Ju': 'checkpoints/RNNInceptionTime_Ju_k_fold_10',\n",
    "    'Si': 'checkpoints/RNNInceptionTime_Si_k_fold_10'\n",
    "}\n",
    "gate_model_dir = 'checkpoints/RNNInceptionTime_Gate_k_fold_10'\n",
    "\n",
    "# Training config\n",
    "k_fold = 10\n",
    "batch_size = 8\n",
    "n_feat = 16\n",
    "n_class = 4\n",
    "window_size = 500\n",
    "max_vgrf_data_len = 25_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_metrics():\n",
    "    metric_names = ['acc', 'f1', 'precision', 'recall']\n",
    "    metrics = {metric_name: {'folds': [], 'avg': None, 'std': None} for metric_name in metric_names}\n",
    "    metrics.update({'cm': {'folds': []}, 'val_loss': {'folds': []}})\n",
    "    return metrics\n",
    "\n",
    "def update_metrics(metrics, in_metrics):\n",
    "    for metric_name in ['acc', 'f1', 'precision', 'recall', 'cm']:\n",
    "        metrics[metric_name]['folds'] += [in_metrics[metric_name]]\n",
    "        if metric_name != 'cm':\n",
    "            metrics[metric_name]['avg'] = np.mean(metrics[metric_name]['folds'])\n",
    "            metrics[metric_name]['std'] = np.std(metrics[metric_name]['folds'])\n",
    "    return metrics\n",
    "\n",
    "moe_metrics = init_metrics()\n",
    "gate_metrics = init_metrics()\n",
    "expert_outputs = {\n",
    "    'Ga': init_metrics(),\n",
    "    'Ju': init_metrics(),\n",
    "    'Si': init_metrics(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_fold in range(k_fold):\n",
    "    print_h(f\"FOLD-{i_fold+1}\", 128)\n",
    "\n",
    "    study_label_map = {\n",
    "        'Ga': 0,\n",
    "        'Ju': 1,\n",
    "        'Si': 2,\n",
    "    }\n",
    "    \n",
    "    expert_model_map = {\n",
    "        'Ga': RNNInceptionTime(c_in=n_feat, c_out=n_class, seq_len=window_size, bidirectional=True).to(device),\n",
    "        'Ju': RNNInceptionTime(c_in=n_feat, c_out=n_class, seq_len=window_size, bidirectional=True).to(device),\n",
    "        'Si': RNNInceptionTime(c_in=n_feat, c_out=n_class, seq_len=window_size, bidirectional=True).to(device),\n",
    "    }\n",
    "\n",
    "    X_train_window_GaJuSi = torch.empty(0, window_size, n_feat).float()\n",
    "    y_train_window_GaJuSi = torch.empty(0).long()\n",
    "    study_labels_train_window_GaJuSi = torch.empty(0).long()\n",
    "    \n",
    "    X_val_window_GaJuSi = torch.empty(0, window_size, n_feat).float()\n",
    "    y_val_window_GaJuSi = torch.empty(0).long()\n",
    "    study_labels_val_window_GaJuSi = torch.empty(0).long()\n",
    "\n",
    "    X_test_window_GaJuSi = torch.empty(0, window_size, n_feat).float()\n",
    "    y_test_window_GaJuSi = torch.empty(0).long()\n",
    "    study_labels_test_window_GaJuSi = torch.empty(0).long()\n",
    "\n",
    "    X_val_person_GaJuSi = torch.empty(0, max_vgrf_data_len, n_feat).float()\n",
    "    y_val_person_GaJuSi = torch.empty(0).long()\n",
    "    # study_labels_val_person_GaJuSi = torch.empty(0).long()\n",
    "\n",
    "    X_test_person_GaJuSi = torch.empty(0, max_vgrf_data_len, n_feat).float()\n",
    "    y_test_person_GaJuSi = torch.empty(0).long()\n",
    "    # study_labels_test_person_GaJuSi = torch.empty(0).long()\n",
    "\n",
    "    for study, k_fold_dir in k_fold_dir_map.items():\n",
    "        fold_i_dir_name = os.listdir(k_fold_dir)[i_fold]\n",
    "        fold_i_dir = os.path.join(k_fold_dir, fold_i_dir_name)\n",
    "\n",
    "        X_train_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'X_train_window.npy'))).float()\n",
    "        y_train_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'y_train_window.npy'))).long()\n",
    "        study_labels_train_window = torch.tensor([study_label_map[study]] * len(y_train_window)).long()\n",
    "        X_train_window_GaJuSi = torch.cat((X_train_window_GaJuSi, X_train_window), dim=0)\n",
    "        y_train_window_GaJuSi = torch.cat((y_train_window_GaJuSi, y_train_window), dim=0)\n",
    "        study_labels_train_window_GaJuSi = torch.cat((study_labels_train_window_GaJuSi, study_labels_train_window), dim=0)\n",
    "\n",
    "        X_val_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'X_val_window.npy'))).float()\n",
    "        y_val_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'y_val_window.npy'))).long()\n",
    "        study_labels_val_window = torch.tensor([study_label_map[study]] * len(y_val_window)).long()\n",
    "        X_val_window_GaJuSi = torch.cat((X_val_window_GaJuSi, X_val_window), dim=0)\n",
    "        y_val_window_GaJuSi = torch.cat((y_val_window_GaJuSi, y_val_window), dim=0)\n",
    "        study_labels_val_window_GaJuSi = torch.cat((study_labels_val_window_GaJuSi, study_labels_val_window), dim=0)\n",
    "\n",
    "        X_test_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'X_test_window.npy'))).float()\n",
    "        y_test_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'y_test_window.npy'))).long()\n",
    "        study_labels_test_window = torch.tensor([study_label_map[study]] * len(y_test_window)).long()\n",
    "        X_test_window_GaJuSi = torch.cat((X_test_window_GaJuSi, X_test_window), dim=0)\n",
    "        y_test_window_GaJuSi = torch.cat((y_test_window_GaJuSi, y_test_window), dim=0)\n",
    "        study_labels_test_window_GaJuSi = torch.cat((study_labels_test_window_GaJuSi, study_labels_test_window), dim=0)\n",
    "\n",
    "        X_val_person = torch.tensor(np.load(os.path.join(fold_i_dir, f'X_val_person.npy'))).float()\n",
    "        y_val_person = torch.tensor(np.load(os.path.join(fold_i_dir, f'y_val_person.npy'))).long()\n",
    "        X_val_person_GaJuSi = torch.cat((X_val_person_GaJuSi, X_val_person), dim=0)\n",
    "        y_val_person_GaJuSi = torch.cat((y_val_person_GaJuSi, y_val_person), dim=0)\n",
    "\n",
    "        X_test_person = torch.tensor(np.load(os.path.join(fold_i_dir, f'X_test_person.npy'))).float()\n",
    "        y_test_person = torch.tensor(np.load(os.path.join(fold_i_dir, f'y_test_person.npy'))).long()\n",
    "        X_test_person_GaJuSi = torch.cat((X_test_person_GaJuSi, X_test_person), dim=0)\n",
    "        y_test_person_GaJuSi = torch.cat((y_test_person_GaJuSi, y_test_person), dim=0)\n",
    "\n",
    "        train_window_dataset = TensorDataset(X_train_window, y_train_window)\n",
    "        val_window_dataset = TensorDataset(X_val_window, y_val_window)\n",
    "        test_window_dataset = TensorDataset(X_test_window, y_test_window)\n",
    "        \n",
    "        val_person_dataset = TensorDataset(X_val_person, y_val_person)\n",
    "        test_person_dataset = TensorDataset(X_test_person, y_test_person)\n",
    "\n",
    "        train_dataloader = DataLoader(train_window_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_window_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_dataloader = DataLoader(test_window_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        expert_model = expert_model_map[study]\n",
    "\n",
    "        # Load pretrained expert model\n",
    "        expert_model_dir = expert_model_dir_map[study]\n",
    "        expert_model_i_name = os.listdir(expert_model_dir)[i_fold]\n",
    "        expert_model_i_path = os.path.join(expert_model_dir, expert_model_i_name)\n",
    "        expert_model.load_state_dict(torch.load(expert_model_i_path))\n",
    "\n",
    "        print_h(\"EVALUATION ON PERSON DATA BY MAJORITY VOTING\", 64)\n",
    "        _, acc_person_majority_voting, f1_person_majority_voting, precision_person_majority_voting, recall_person_majority_voting, cm_person_majority_voting = eval_person_majority_voting(expert_model, val_person_dataset, criterion=None, average='weighted',\n",
    "                                                                                                                                                                                                window_size=window_size, debug=False)\n",
    "        print(\"acc:\", acc_person_majority_voting)\n",
    "        print(\"f1:\", f1_person_majority_voting)\n",
    "        print(\"precision:\", precision_person_majority_voting)\n",
    "        print(\"recall:\", recall_person_majority_voting)\n",
    "        print(\"cm:\\n\", np.array(cm_person_majority_voting))\n",
    "        print()\n",
    "\n",
    "        expert_outputs[study] = update_metrics(expert_outputs[study], {\n",
    "            'acc': acc_person_majority_voting,\n",
    "            'f1': f1_person_majority_voting,\n",
    "            'precision': precision_person_majority_voting,\n",
    "            'recall': recall_person_majority_voting,\n",
    "            'cm': cm_person_majority_voting,\n",
    "        })\n",
    "\n",
    "    print_h(\"GATE\", 96)\n",
    "\n",
    "    # train_window_dataset_GaJuSi = TensorDataset(X_train_window_GaJuSi, y_train_window_GaJuSi)\n",
    "    # val_window_dataset_GaJuSi = TensorDataset(X_val_window_GaJuSi, y_val_window_GaJuSi)\n",
    "    # test_window_dataset_GaJuSi = TensorDataset(X_test_window_GaJuSi, y_test_window_GaJuSi)\n",
    "\n",
    "    train_window_dataset_GaJuSi = TensorDataset(X_train_window_GaJuSi, study_labels_train_window_GaJuSi)\n",
    "    val_window_dataset_GaJuSi = TensorDataset(X_val_window_GaJuSi, study_labels_val_window_GaJuSi)\n",
    "    test_window_dataset_GaJuSi = TensorDataset(X_test_window_GaJuSi, study_labels_test_window_GaJuSi)\n",
    "\n",
    "    train_dataloader_GaJuSi = DataLoader(train_window_dataset_GaJuSi, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader_GaJuSi = DataLoader(val_window_dataset_GaJuSi, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader_GaJuSi = DataLoader(test_window_dataset_GaJuSi, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    gate_model = RNNInceptionTime(c_in=n_feat, c_out=len(study_label_map.keys()), seq_len=window_size, bidirectional=True).to(device)\n",
    "\n",
    "    # Load pretrained gate model\n",
    "    gate_model_i_name = os.listdir(gate_model_dir)[i_fold]\n",
    "    gate_model_i_path = os.path.join(gate_model_dir, gate_model_i_name)\n",
    "    gate_model.load_state_dict(torch.load(gate_model_i_path))\n",
    "\n",
    "    print_h(\"EVALUATION ON WINDOW DATA\", 64)\n",
    "    \n",
    "    _, acc_window, f1_window, precision_window, recall_window, cm_window = eval_window(gate_model, test_dataloader_GaJuSi, average='weighted')\n",
    "\n",
    "    print(\"acc:\", acc_window)\n",
    "    print(\"f1:\", f1_window)\n",
    "    print(\"precision:\", precision_window)\n",
    "    print(\"recall:\", recall_window)\n",
    "    print(\"cm:\\n\", np.array(cm_window))\n",
    "    print()\n",
    "\n",
    "    gate_metrics = update_metrics(gate_metrics, {\n",
    "        'acc': acc_window,\n",
    "        'f1': f1_window,\n",
    "        'precision': precision_window,\n",
    "        'recall': recall_window,\n",
    "        'cm': cm_window,\n",
    "    })\n",
    "\n",
    "    print_h(\"MoE\", 96)\n",
    "\n",
    "    val_person_dataset_GaJuSi = TensorDataset(X_val_person_GaJuSi, y_val_person_GaJuSi)\n",
    "    test_person_dataset_GaJuSi = TensorDataset(X_test_person_GaJuSi, y_test_person_GaJuSi)\n",
    "\n",
    "    moe_model = HardMoE(experts=expert_model_map.values(), gate=gate_model)\n",
    "\n",
    "    print_h(\"EVALUATION ON PERSON DATA BY MAJORITY VOTING\", 64)\n",
    "    _, acc_person_majority_voting, f1_person_majority_voting, precision_person_majority_voting, recall_person_majority_voting, cm_person_majority_voting = eval_person_majority_voting(moe_model, val_person_dataset_GaJuSi, criterion=None, average='weighted',\n",
    "                                                                                                                                                                                        window_size=window_size, debug=False)\n",
    "    print(\"acc:\", acc_person_majority_voting)\n",
    "    print(\"f1:\", f1_person_majority_voting)\n",
    "    print(\"precision:\", precision_person_majority_voting)\n",
    "    print(\"recall:\", recall_person_majority_voting)\n",
    "    print(\"cm:\\n\", np.array(cm_person_majority_voting))\n",
    "    print()\n",
    "\n",
    "    moe_metrics = update_metrics(moe_metrics, {\n",
    "        'acc': acc_person_majority_voting,\n",
    "        'f1': f1_person_majority_voting,\n",
    "        'precision': precision_person_majority_voting,\n",
    "        'recall': recall_person_majority_voting,\n",
    "        'cm': cm_person_majority_voting,\n",
    "    })\n",
    "\n",
    "    # for metric in ['acc', 'f1', 'precision', 'recall', 'cm']:\n",
    "    #     moe_metrics[metric]['folds'] += [moe_eval_output[metric]]\n",
    "    #     if metric != 'cm':\n",
    "    #         moe_metrics[metric]['avg'] = np.mean(moe_metrics[metric]['folds'])\n",
    "    #         moe_metrics[metric]['std'] = np.std(moe_metrics[metric]['folds'])\n",
    "    \n",
    "    # output['window']['train_loss']['folds'].append(global_train_loss_list)\n",
    "    # output['window']['val_loss']['folds'].append(global_val_loss_window_list)\n",
    "\n",
    "    # break # Test for only 1 fold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
