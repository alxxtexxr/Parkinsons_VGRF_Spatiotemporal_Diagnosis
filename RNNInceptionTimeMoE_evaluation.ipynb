{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactored from: https://github.com/alxxtexxr/Parkinsons_VGRF_Spatiotemporal_Diagnosis/blob/master/eval_RNNInceptionTimeMoe_kfold10.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo 'Parkinsons_VGRF_Spatiotemporal_Diagnosis_v2' already exists. Updating to the latest version...\n",
      "Now working at repo directory: /content/Parkinsons_VGRF_Spatiotemporal_Diagnosis_v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def sparse_clone_repo(repo_url, sparse_dirs, root_dir='.'):\n",
    "    \"\"\"\n",
    "    Clones or updates a Git repository with sparse checkout for specified directories.\n",
    "    Parameters:\n",
    "    - repo_url (str): The URL of the Git repository.\n",
    "    - sparse_dirs (list of str): List of directories to retrieve using sparse checkout.\n",
    "    - root_dir (str): The root directory where the repository will be stored (default is '/content').\n",
    "    \"\"\"\n",
    "    repo_name = repo_url.rstrip('.git').split('/')[-1] # Extract repo name from URL\n",
    "    repo_dir = os.path.join(root_dir, repo_name)\n",
    "\n",
    "    if os.path.isdir(repo_dir):\n",
    "        print(f\"Repo '{repo_name}' already exists. Updating to the latest version...\")\n",
    "        os.chdir(repo_dir)\n",
    "        subprocess.run(['git', 'reset', '--hard', 'origin/master'], check=True) # Reset to match remote\n",
    "        subprocess.run(['git', 'pull'], check=True) # Pull latest changes\n",
    "    else:\n",
    "        print(f\"Repo '{repo_name}' not found. Cloning a fresh copy...\")\n",
    "        os.chdir(root_dir)\n",
    "        subprocess.run(['git', 'clone', '--no-checkout', repo_url], check=True)\n",
    "        os.chdir(repo_dir)\n",
    "        print(f\"Initializing sparse checkout for directories: {', '.join(sparse_dirs)}...\")\n",
    "        subprocess.run(['git', 'sparse-checkout', 'init', '--cone'], check=True)\n",
    "        subprocess.run(['git', 'sparse-checkout', 'set', *sparse_dirs], check=True)\n",
    "        subprocess.run(['git', 'checkout'], check=True)\n",
    "    \n",
    "    print(\"Now working at repo directory:\", os.getcwd())\n",
    "\n",
    "sparse_clone_repo(\n",
    "    repo_url='https://github.com/alxxtexxr/Parkinsons_VGRF_Spatiotemporal_Diagnosis_v2.git',\n",
    "    sparse_dirs=['src', 'data', 'checkpoints'],\n",
    "    root_dir='/content'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install tsai # Required library for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from src.utils import (\n",
    "    print_h, eval_window, eval_person_majority_voting,\n",
    "    set_seed,\n",
    ")\n",
    "from src.models import RNNInceptionTime, HardMoE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 69\n"
     ]
    }
   ],
   "source": [
    "# Project config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 69\n",
    "set_seed(seed)\n",
    "\n",
    "# Model config\n",
    "k_fold_dir_map = {\n",
    "    'Ga': 'data/preprocessed_LOOCV/Ga_k_fold_10_window_500_stride_500_n_feat_16',\n",
    "    'Ju': 'data/preprocessed_LOOCV/Ju_k_fold_10_window_500_stride_500_n_feat_16',\n",
    "    'Si': 'data/preprocessed_LOOCV/Si_k_fold_10_window_500_stride_250_n_feat_16',\n",
    "}\n",
    "expert_model_dir_map = {\n",
    "    'Ga': 'checkpoints/RNNInceptionTime_Ga_k_fold_10',\n",
    "    'Ju': 'checkpoints/RNNInceptionTime_Ju_k_fold_10',\n",
    "    'Si': 'checkpoints/RNNInceptionTime_Si_k_fold_10'\n",
    "}\n",
    "gate_model_dir = 'checkpoints/RNNInceptionTime_Gate_k_fold_10'\n",
    "\n",
    "# Training config\n",
    "k_fold = 10\n",
    "batch_size = 8\n",
    "n_feat = 16\n",
    "n_class = 4\n",
    "window_size = 500\n",
    "max_vgrf_data_len = 25_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_metrics():\n",
    "    metric_names = ['acc', 'f1', 'precision', 'recall']\n",
    "    metrics = {metric_name: {'folds': [], 'avg': None, 'std': None} for metric_name in metric_names}\n",
    "    metrics.update({'cm': {'folds': []}, 'val_loss': {'folds': []}})\n",
    "    return metrics\n",
    "\n",
    "def update_metrics(metrics, in_metrics):\n",
    "    for metric_name in ['acc', 'f1', 'precision', 'recall', 'cm']:\n",
    "        metrics[metric_name]['folds'] += [in_metrics[metric_name]]\n",
    "        if metric_name != 'cm':\n",
    "            metrics[metric_name]['avg'] = np.mean(metrics[metric_name]['folds'])\n",
    "            metrics[metric_name]['std'] = np.std(metrics[metric_name]['folds'])\n",
    "    return metrics\n",
    "\n",
    "moe_metrics = init_metrics()\n",
    "gate_metrics = init_metrics()\n",
    "expert_outputs = {\n",
    "    'Ga': init_metrics(),\n",
    "    'Ju': init_metrics(),\n",
    "    'Si': init_metrics(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================================================================\n",
      "                                                             FOLD-1                                                             \n",
      "================================================================================================================================\n",
      "data/preprocessed_LOOCV/Ga_k_fold_10_window_500_stride_500_n_feat_16/fold_3/X_train_window.npy\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "No data left in file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f536443b31eb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_i_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'X_train_window.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mX_train_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_i_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'X_train_window.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0my_train_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_i_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'y_train_window.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mstudy_labels_train_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstudy_label_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mmagic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmagic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No data left in file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0;31m# If the file size is less than N, we need to make sure not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;31m# to seek past the beginning of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: No data left in file"
     ]
    }
   ],
   "source": [
    "study_label_map = {\n",
    "    'Ga': 0,\n",
    "    'Ju': 1,\n",
    "    'Si': 2,\n",
    "}\n",
    "\n",
    "for i_fold in range(k_fold):\n",
    "    print_h(f\"FOLD-{i_fold+1}\", 128)\n",
    "    \n",
    "    expert_model_map = {\n",
    "        'Ga': RNNInceptionTime(c_in=n_feat, c_out=n_class, seq_len=window_size, bidirectional=True).to(device),\n",
    "        'Ju': RNNInceptionTime(c_in=n_feat, c_out=n_class, seq_len=window_size, bidirectional=True).to(device),\n",
    "        'Si': RNNInceptionTime(c_in=n_feat, c_out=n_class, seq_len=window_size, bidirectional=True).to(device),\n",
    "    }\n",
    "\n",
    "    X_train_window_GaJuSi = torch.empty(0, window_size, n_feat).float()\n",
    "    y_train_window_GaJuSi = torch.empty(0).long()\n",
    "    study_labels_train_window_GaJuSi = torch.empty(0).long()\n",
    "    \n",
    "    X_val_window_GaJuSi = torch.empty(0, window_size, n_feat).float()\n",
    "    y_val_window_GaJuSi = torch.empty(0).long()\n",
    "    study_labels_val_window_GaJuSi = torch.empty(0).long()\n",
    "\n",
    "    X_test_window_GaJuSi = torch.empty(0, window_size, n_feat).float()\n",
    "    y_test_window_GaJuSi = torch.empty(0).long()\n",
    "    study_labels_test_window_GaJuSi = torch.empty(0).long()\n",
    "\n",
    "    X_val_person_GaJuSi = torch.empty(0, max_vgrf_data_len, n_feat).float()\n",
    "    y_val_person_GaJuSi = torch.empty(0).long()\n",
    "    # study_labels_val_person_GaJuSi = torch.empty(0).long()\n",
    "\n",
    "    X_test_person_GaJuSi = torch.empty(0, max_vgrf_data_len, n_feat).float()\n",
    "    y_test_person_GaJuSi = torch.empty(0).long()\n",
    "    # study_labels_test_person_GaJuSi = torch.empty(0).long()\n",
    "\n",
    "    for study, k_fold_dir in k_fold_dir_map.items():\n",
    "        fold_i_dir_name = os.listdir(k_fold_dir)[i_fold]\n",
    "        fold_i_dir = os.path.join(k_fold_dir, fold_i_dir_name)\n",
    "\n",
    "        print(os.path.join(fold_i_dir, f'X_train_window.npy'))\n",
    "        X_train_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'X_train_window.npy'))).float()\n",
    "        y_train_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'y_train_window.npy'))).long()\n",
    "        study_labels_train_window = torch.tensor([study_label_map[study]] * len(y_train_window)).long()\n",
    "        X_train_window_GaJuSi = torch.cat((X_train_window_GaJuSi, X_train_window), dim=0)\n",
    "        y_train_window_GaJuSi = torch.cat((y_train_window_GaJuSi, y_train_window), dim=0)\n",
    "        study_labels_train_window_GaJuSi = torch.cat((study_labels_train_window_GaJuSi, study_labels_train_window), dim=0)\n",
    "\n",
    "        X_val_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'X_val_window.npy'))).float()\n",
    "        y_val_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'y_val_window.npy'))).long()\n",
    "        study_labels_val_window = torch.tensor([study_label_map[study]] * len(y_val_window)).long()\n",
    "        X_val_window_GaJuSi = torch.cat((X_val_window_GaJuSi, X_val_window), dim=0)\n",
    "        y_val_window_GaJuSi = torch.cat((y_val_window_GaJuSi, y_val_window), dim=0)\n",
    "        study_labels_val_window_GaJuSi = torch.cat((study_labels_val_window_GaJuSi, study_labels_val_window), dim=0)\n",
    "\n",
    "        X_test_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'X_test_window.npy'))).flo2t()\n",
    "        y_test_window = torch.tensor(np.load(os.path.join(fold_i_dir, f'y_test_window.npy'))).long()\n",
    "        study_labels_test_window = torch.tensor([study_label_map[study]] * len(y_test_window)).long()\n",
    "        X_test_window_GaJuSi = torch.cat((X_test_window_GaJuSi, X_test_window), dim=0)\n",
    "        y_test_window_GaJuSi = torch.cat((y_test_window_GaJuSi, y_test_window), dim=0)\n",
    "        study_labels_test_window_GaJuSi = torch.cat((study_labels_test_window_GaJuSi, study_labels_test_window), dim=0)\n",
    "\n",
    "        X_val_person = torch.tensor(np.load(os.path.join(fold_i_dir, f'X_val_person.npy'))).float()\n",
    "        y_val_person = torch.tensor(np.load(os.path.join(fold_i_dir, f'y_val_person.npy'))).long()\n",
    "        X_val_person_GaJuSi = torch.cat((X_val_person_GaJuSi, X_val_person), dim=0)\n",
    "        y_val_person_GaJuSi = torch.cat((y_val_person_GaJuSi, y_val_person), dim=0)\n",
    "\n",
    "        X_test_person = torch.tensor(np.load(os.path.join(fold_i_dir, f'X_test_person.npy'))).float()\n",
    "        y_test_person = torch.tensor(np.load(os.path.join(fold_i_dir, f'y_test_person.npy'))).long()\n",
    "        X_test_person_GaJuSi = torch.cat((X_test_person_GaJuSi, X_test_person), dim=0)\n",
    "        y_test_person_GaJuSi = torch.cat((y_test_person_GaJuSi, y_test_person), dim=0)\n",
    "\n",
    "        train_window_dataset = TensorDataset(X_train_window, y_train_window)\n",
    "        val_window_dataset = TensorDataset(X_val_window, y_val_window)\n",
    "        test_window_dataset = TensorDataset(X_test_window, y_test_window)\n",
    "        \n",
    "        val_person_dataset = TensorDataset(X_val_person, y_val_person)\n",
    "        test_person_dataset = TensorDataset(X_test_person, y_test_person)\n",
    "\n",
    "        train_dataloader = DataLoader(train_window_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_window_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_dataloader = DataLoader(test_window_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        expert_model = expert_model_map[study]\n",
    "\n",
    "        # Load pretrained expert model\n",
    "        expert_model_dir = expert_model_dir_map[study]\n",
    "        expert_model_i_name = os.listdir(expert_model_dir)[i_fold]\n",
    "        expert_model_i_path = os.path.join(expert_model_dir, expert_model_i_name)\n",
    "        expert_model.load_state_dict(torch.load(expert_model_i_path))\n",
    "\n",
    "        print_h(\"EVALUATION ON PERSON DATA BY MAJORITY VOTING\", 64)\n",
    "        _, acc_person_majority_voting, f1_person_majority_voting, precision_person_majority_voting, recall_person_majority_voting, cm_person_majority_voting = eval_person_majority_voting(expert_model, val_person_dataset, criterion=None, average='weighted',\n",
    "                                                                                                                                                                                                window_size=window_size, debug=False)\n",
    "        print(\"acc:\", acc_person_majority_voting)\n",
    "        print(\"f1:\", f1_person_majority_voting)\n",
    "        print(\"precision:\", precision_person_majority_voting)\n",
    "        print(\"recall:\", recall_person_majority_voting)\n",
    "        print(\"cm:\\n\", np.array(cm_person_majority_voting))\n",
    "        print()\n",
    "\n",
    "        expert_outputs[study] = update_metrics(expert_outputs[study], {\n",
    "            'acc': acc_person_majority_voting,\n",
    "            'f1': f1_person_majority_voting,\n",
    "            'precision': precision_person_majority_voting,\n",
    "            'recall': recall_person_majority_voting,\n",
    "            'cm': cm_person_majority_voting,\n",
    "        })\n",
    "\n",
    "    print_h(\"GATE\", 96)\n",
    "\n",
    "    # train_window_dataset_GaJuSi = TensorDataset(X_train_window_GaJuSi, y_train_window_GaJuSi)\n",
    "    # val_window_dataset_GaJuSi = TensorDataset(X_val_window_GaJuSi, y_val_window_GaJuSi)\n",
    "    # test_window_dataset_GaJuSi = TensorDataset(X_test_window_GaJuSi, y_test_window_GaJuSi)\n",
    "\n",
    "    train_window_dataset_GaJuSi = TensorDataset(X_train_window_GaJuSi, study_labels_train_window_GaJuSi)\n",
    "    val_window_dataset_GaJuSi = TensorDataset(X_val_window_GaJuSi, study_labels_val_window_GaJuSi)\n",
    "    test_window_dataset_GaJuSi = TensorDataset(X_test_window_GaJuSi, study_labels_test_window_GaJuSi)\n",
    "\n",
    "    train_dataloader_GaJuSi = DataLoader(train_window_dataset_GaJuSi, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader_GaJuSi = DataLoader(val_window_dataset_GaJuSi, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader_GaJuSi = DataLoader(test_window_dataset_GaJuSi, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    gate_model = RNNInceptionTime(c_in=n_feat, c_out=len(study_label_map.keys()), seq_len=window_size, bidirectional=True).to(device)\n",
    "\n",
    "    # Load pretrained gate model\n",
    "    gate_model_i_name = os.listdir(gate_model_dir)[i_fold]\n",
    "    gate_model_i_path = os.path.join(gate_model_dir, gate_model_i_name)\n",
    "    gate_model.load_state_dict(torch.load(gate_model_i_path))\n",
    "\n",
    "    print_h(\"EVALUATION ON WINDOW DATA\", 64)\n",
    "    \n",
    "    _, acc_window, f1_window, precision_window, recall_window, cm_window = eval_window(gate_model, test_dataloader_GaJuSi, average='weighted')\n",
    "\n",
    "    print(\"acc:\", acc_window)\n",
    "    print(\"f1:\", f1_window)\n",
    "    print(\"precision:\", precision_window)\n",
    "    print(\"recall:\", recall_window)\n",
    "    print(\"cm:\\n\", np.array(cm_window))\n",
    "    print()\n",
    "\n",
    "    gate_metrics = update_metrics(gate_metrics, {\n",
    "        'acc': acc_window,\n",
    "        'f1': f1_window,\n",
    "        'precision': precision_window,\n",
    "        'recall': recall_window,\n",
    "        'cm': cm_window,\n",
    "    })\n",
    "\n",
    "    print_h(\"MoE\", 96)\n",
    "\n",
    "    val_person_dataset_GaJuSi = TensorDataset(X_val_person_GaJuSi, y_val_person_GaJuSi)\n",
    "    test_person_dataset_GaJuSi = TensorDataset(X_test_person_GaJuSi, y_test_person_GaJuSi)\n",
    "\n",
    "    moe_model = HardMoE(experts=expert_model_map.values(), gate=gate_model)\n",
    "\n",
    "    print_h(\"EVALUATION ON PERSON DATA BY MAJORITY VOTING\", 64)\n",
    "    _, acc_person_majority_voting, f1_person_majority_voting, precision_person_majority_voting, recall_person_majority_voting, cm_person_majority_voting = eval_person_majority_voting(moe_model, val_person_dataset_GaJuSi, criterion=None, average='weighted',\n",
    "                                                                                                                                                                                        window_size=window_size, debug=False)\n",
    "    print(\"acc:\", acc_person_majority_voting)\n",
    "    print(\"f1:\", f1_person_majority_voting)\n",
    "    print(\"precision:\", precision_person_majority_voting)\n",
    "    print(\"recall:\", recall_person_majority_voting)\n",
    "    print(\"cm:\\n\", np.array(cm_person_majority_voting))\n",
    "    print()\n",
    "\n",
    "    moe_metrics = update_metrics(moe_metrics, {\n",
    "        'acc': acc_person_majority_voting,\n",
    "        'f1': f1_person_majority_voting,\n",
    "        'precision': precision_person_majority_voting,\n",
    "        'recall': recall_person_majority_voting,\n",
    "        'cm': cm_person_majority_voting,\n",
    "    })\n",
    "\n",
    "    # for metric in ['acc', 'f1', 'precision', 'recall', 'cm']:\n",
    "    #     moe_metrics[metric]['folds'] += [moe_eval_output[metric]]\n",
    "    #     if metric != 'cm':\n",
    "    #         moe_metrics[metric]['avg'] = np.mean(moe_metrics[metric]['folds'])\n",
    "    #         moe_metrics[metric]['std'] = np.std(moe_metrics[metric]['folds'])\n",
    "    \n",
    "    # output['window']['train_loss']['folds'].append(global_train_loss_list)\n",
    "    # output['window']['val_loss']['folds'].append(global_val_loss_window_list)\n",
    "\n",
    "    # break # Test for only 1 fold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
